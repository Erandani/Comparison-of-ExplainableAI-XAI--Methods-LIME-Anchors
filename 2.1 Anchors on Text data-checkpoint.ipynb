{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dataset : Twitter data from travelers in US Airline, in February 2015 which expressed their feelings on service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#for NLP\n",
    "import re    \n",
    "import nltk    \n",
    "from textblob import TextBlob    \n",
    "from nltk.corpus import stopwords    \n",
    "from nltk.stem import PorterStemmer    \n",
    "from textblob import Word    \n",
    "from nltk.util import ngrams    \n",
    "from wordcloud import WordCloud, STOPWORDS    \n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import spacy\n",
    "\n",
    "#for Anchor\n",
    "from alibi.explainers import AnchorText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert words to lowercase\n",
    "df['text'] = df['text'].apply(lambda x: \" \".join(x.lower() for x in x.split())) \n",
    "    \n",
    "#Removes unicode strings like \"\\u002c\" and \"x96\"\n",
    "df['text']= df['text'].str.replace(r'(\\\\u[0-9A-Fa-f]+)','')\n",
    "df['text'] = df['text'].str.replace(r'[^\\x00-\\x7f]','')\n",
    "    \n",
    "#convert any url to URL\n",
    "df['text'] = df['text'].str.replace('((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+))','URL')\n",
    "    \n",
    "#Convert any @Username to \"AT_USER\"\n",
    "df['text'] = df['text'].str.replace('@[^\\s]+','')\n",
    "    \n",
    "#Remove additional white spaces\n",
    "df['text'] = df['text'].str.replace('[\\s]+',' ')\n",
    "df['text'] = df['text'].str.replace('[\\n]+',' ')\n",
    "    \n",
    "#Remove not alphanumeric symbols white spaces\n",
    "df['text']= df['text'].str.replace(r'[^\\w]', ' ')\n",
    "    \n",
    "#Removes hastag in front of a word \"\"\"\n",
    "df['text']= df['text'].str.replace(r'#([^\\s]+)', r'\\1')\n",
    "    \n",
    "#Removes:) or :(\n",
    "df['text']= df['text'].str.replace(r':\\)',\"\")\n",
    "df['text']= df['text'].str.replace(r':\\(',\"\")\n",
    "    \n",
    "#remove numbers\n",
    "df['text']= df['text'].apply(lambda x: \" \".join(x for x in x.split() if not x.isdigit()))\n",
    "    \n",
    "#remove multiple exclamation\n",
    "df['text']= df['text'].str.replace(r\"(\\!)\\1+\", ' ')\n",
    "    \n",
    "#remove multiple question marks\n",
    "df['text']= df['text'].str.replace(r\"(\\?)\\1+\", ' ')\n",
    "    \n",
    "#remove multistop\n",
    "df['text']= df['text'].str.replace(r\"(\\.)\\1+\", ' ')\n",
    "    \n",
    "#lemma\n",
    "from textblob import Word\n",
    "df['text']= df['text'].apply(lambda x: \" \".join(Word(word).lemmatize() for word in x.split())) \n",
    "    \n",
    "#Removes emoticons from text\n",
    "df['text']= df['text'].str.replace(':\\)|;\\)|:-\\)|\\(-:|:-D|=D|:P|xD|X-p|\\^\\^|:-*|\\^\\.\\^|\\^\\-\\^|\\^\\_\\^|\\,-\\)|\\)-:|:\\'\\(|:\\(|:-\\(|:\\S|T\\.T|\\.\\_\\.|:<|:-\\S|:-<|\\*\\-\\*|:O|=O|=\\-O|O\\.o|XO|O\\_O|:-\\@|=/|:/|X\\-\\(|>\\.<|>=\\(|D:', \"r\")\n",
    "    \n",
    "#trim\n",
    "df['text']= df['text'].str.strip('\\'\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['airline_sentiment'].apply(lambda x:0 if x=='negative' else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['text'],y, test_size=0.2, stratify=y, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vect = vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Fitting in a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Pipeline([('tidf',TfidfVectorizer()), ('NB', BernoulliNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('tidf',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 sublinear_tf=False,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, use_idf=True,\n",
       "                                 vocabulary=None)),\n",
       "                ('NB',\n",
       "                 BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None,\n",
       "                             fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_fn = lambda x: clf.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy 87.713456284153\n",
      "Test accuracy 83.40163934426229\n"
     ]
    }
   ],
   "source": [
    "preds_train = predict_fn(X_train)\n",
    "preds_test = predict_fn(X_test)\n",
    "\n",
    "print('Train accuracy', accuracy_score(y_train, preds_train)*100)\n",
    "print('Test accuracy', accuracy_score(y_test, preds_test)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constructing Anchors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load spaCy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize anchor text explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = AnchorText(nlp, predict_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names =[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i m glad you re sorry that i m homeless for the night make me feel secure\n"
     ]
    }
   ],
   "source": [
    "i=191\n",
    "text = X_test.iloc[i]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True label: 0\n",
      "Naive Bayes model prediction label: 0\n",
      "Naive Bayes model prediction probability for 0: 0.5037463665917669\n"
     ]
    }
   ],
   "source": [
    "pred = class_names[predict_fn([text])[0]]\n",
    "alternative = class_names[1 - predict_fn([text])[0]]\n",
    "\n",
    "print('True label:', y_test.iloc[i])\n",
    "print('Naive Bayes model prediction label:',pred)\n",
    "print('Naive Bayes model prediction probability for 0:',clf.predict_proba(X_test)[i][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prediction probability is 0.504, it is near to the decision boundary. \n",
    "\n",
    "Lets examine how Anchors will be stated to capture 0(negative nature of the prediction) correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explanation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.41 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "explanation = explainer.explain(text, threshold=0.95, use_unk=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use_unk=True means we will perturb examples by replacing words with UNKs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anchor: night AND homeless AND sorry AND that AND feel AND for AND me AND re AND make AND the AND secure\n",
      "Precision: 1.00\n"
     ]
    }
   ],
   "source": [
    "print('Anchor: %s' % (' AND '.join(explanation.anchor)))\n",
    "print('Precision: %.2f' % explanation.precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial Anchor: night AND homeless AND sorry\n"
     ]
    }
   ],
   "source": [
    "print('Partial Anchor: %s' % (' AND '.join(explanation.anchor[0:3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now take a look at the partial anchor which extracts top most conditions.The words 'night', 'homeless' and 'sorry' condfidently guarantees a negative prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Examples where anchor applies and model predicts 0:\n",
      "i UNK glad you re sorry that UNK UNK homeless for the night make me feel secure\n",
      "UNK UNK UNK you re sorry that i UNK homeless for the night make me feel secure\n",
      "UNK UNK glad UNK re sorry that UNK UNK homeless for the night make me feel secure\n",
      "UNK m UNK you re sorry that UNK m homeless for the night make me feel secure\n",
      "UNK m glad you re sorry that i m homeless for the night make me feel secure\n",
      "i m UNK you re sorry that UNK UNK homeless for the night make me feel secure\n",
      "UNK UNK UNK UNK re sorry that i m homeless for the night make me feel secure\n",
      "UNK m glad UNK re sorry that UNK UNK homeless for the night make me feel secure\n",
      "UNK UNK glad you re sorry that UNK m homeless for the night make me feel secure\n",
      "UNK m glad UNK re sorry that UNK UNK homeless for the night make me feel secure\n"
     ]
    }
   ],
   "source": [
    "print('\\nExamples where anchor applies and model predicts %s:' % pred)\n",
    "print('\\n'.join([j for j in explanation.raw['examples'][-1]['covered_true']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Examples where anchor applies and model predicts 1:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\nExamples where anchor applies and model predicts %s:' % alternative)\n",
    "print('\\n'.join([p for p in explanation.raw['examples'][-1]['covered_false']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no perturbated senarios where those Anchor words apply and model predicts sentiment as positive(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
